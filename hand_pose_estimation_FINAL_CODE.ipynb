{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNvmyADzbY7Y"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2kFz_77lqlHo",
    "outputId": "317a2896-3f71-4b73-88a0-43927ab7e495"
   },
   "outputs": [],
   "source": [
    "!pip install -q mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0_AAXQ84xuW"
   },
   "outputs": [],
   "source": [
    "!wget -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yASdWaOHraTP"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 977
    },
    "id": "e6Efd4tYrYZZ",
    "outputId": "16d06bc8-f1ea-4296-fd6b-3e2ea851c4db"
   },
   "outputs": [],
   "source": [
    "!wget -q -O image.jpg https://storage.googleapis.com/mediapipe-tasks/hand_landmarker/woman_hands.jpg\n",
    "\n",
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "img = cv2.imread(\"image.jpg\")\n",
    "cv2_imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGHbjkE3rY8B"
   },
   "outputs": [],
   "source": [
    "# STEP 1: Import the necessary modules.\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# STEP 2: Create an HandLandmarker object.\n",
    "base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
    "options = vision.HandLandmarkerOptions(base_options=base_options,\n",
    "                                       num_hands=2)\n",
    "detector = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "im_name = \"image.jpg\"\n",
    "# STEP 3: Load the input image.\n",
    "image = mp.Image.create_from_file(im_name)\n",
    "\n",
    "# STEP 4: Detect hand landmarks from the input image.\n",
    "detection_result = detector.detect(image)\n",
    "\n",
    "# STEP 5: Process the classification result. In this case, visualize it.\n",
    "# annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
    "# cv2_imshow(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15ljJPok5qDs"
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for hand in detection_result.hand_landmarks:\n",
    "  for landmark in hand:\n",
    "    x.append(landmark.x)\n",
    "    y.append(landmark.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 977
    },
    "id": "QYEHHJL35V9t",
    "outputId": "4a5b5efc-93a8-41b5-e731-34e7a22667d2"
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(im_name)\n",
    "# print(img)\n",
    "\n",
    "for i in range(len(x)):\n",
    "  x_coor = int(x[i] * img.shape[1])\n",
    "  y_coor = int(y[i] * img.shape[0])\n",
    "  cv2.circle(img, (x_coor, y_coor), 5, (0, 255, 0), -1)\n",
    "\n",
    "cv2_imshow(img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFyDcEd3D-iJ"
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aikyewd0J13K"
   },
   "outputs": [],
   "source": [
    "# step 1: change this based on my code\n",
    "def str_to_index(str):\n",
    "  if(str == 'thumb_base'):\n",
    "    index = 2\n",
    "  elif(str == 'thumb_mp'):\n",
    "    index = 3\n",
    "  elif(str == 'thumb_tip'):\n",
    "    index = 4\n",
    "  elif(str == 'index_base'):\n",
    "    index = 5\n",
    "  elif(str == 'index_mp'):\n",
    "    index = 7\n",
    "  elif(str == 'index_tip'):\n",
    "    index = 8\n",
    "  elif(str == 'middle_base'):\n",
    "    index = 9\n",
    "  elif(str == 'middle_mp'):\n",
    "    index = 11\n",
    "  elif(str == 'middle_tip'):\n",
    "    index = 12\n",
    "  elif(str == 'ring_base'):\n",
    "    index = 13\n",
    "  elif(str == 'ring_mp'):\n",
    "    index = 15\n",
    "  elif(str == 'ring_tip'):\n",
    "    index = 16\n",
    "  elif(str == 'pinkie_base'):\n",
    "    index = 17\n",
    "  elif(str == 'pinkie_mp'):\n",
    "    index = 19\n",
    "  elif(str == 'pinkie_tip'):\n",
    "    index = 20\n",
    "  elif(str == 'wrist'):\n",
    "    index = 0\n",
    "\n",
    "\n",
    "  return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JZdqQ3uGokq"
   },
   "outputs": [],
   "source": [
    "def draw_keypoints(im_name, L_x,L_y,R_x,R_y):\n",
    "  print(im_name)\n",
    "\n",
    "  img = cv2.imread(im_name)\n",
    "  # cv2_imshow(img)\n",
    "\n",
    "  for i in range(len(L_x)):\n",
    "    x_coor = int(L_x[i] * img.shape[1]/100)\n",
    "    y_coor = int(L_y[i] * img.shape[0]/100)\n",
    "    if(x_coor == -1):\n",
    "      pass\n",
    "    cv2.circle(img, (x_coor, y_coor), 2, (0, 255, 0), -1)\n",
    "\n",
    "  for i in range(len(R_x)):\n",
    "    x_coor = int(R_x[i] * img.shape[1]/100)\n",
    "    y_coor = int(R_y[i] * img.shape[0]/100)\n",
    "    if(x_coor == -1):\n",
    "      pass\n",
    "    cv2.circle(img, (x_coor, y_coor), 2, (0, 0, 255), -1)\n",
    "\n",
    "  cv2_imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XSUEVRZGJ0_L"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1SRI45H1E0Ml"
   },
   "outputs": [],
   "source": [
    "class Annotation:\n",
    "    def __init__(self, image_name, L_keypoint_label_arr, L_x_arr, L_y_arr, \\\n",
    "                 R_keypoint_label_arr, R_x_arr, R_y_arr):\n",
    "        self.image_name = image_name\n",
    "\n",
    "        self.L_keypoint_label_arr = L_keypoint_label_arr\n",
    "        self.L_x_arr = L_x_arr\n",
    "        self.L_y_arr = L_y_arr\n",
    "\n",
    "        self.R_keypoint_label_arr = R_keypoint_label_arr\n",
    "        self.R_x_arr = R_x_arr\n",
    "        self.R_y_arr = R_y_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c7v-13n8D6qS"
   },
   "outputs": [],
   "source": [
    "NUM_HAND_POINTS = 21\n",
    "\n",
    "with open('more_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract x and y coordinates\n",
    "coordinates = []\n",
    "annotations = []\n",
    "for item in data:\n",
    "\n",
    "    image_name = item[\"data\"][\"img\"]\n",
    "\n",
    "    cut_index = image_name.find(\"-\")\n",
    "    if cut_index != -1:\n",
    "      new_image_name = image_name[cut_index + len(\"-\"):]\n",
    "\n",
    "    L_x_coors = np.ones(NUM_HAND_POINTS)*-1\n",
    "    L_y_coors = np.ones(NUM_HAND_POINTS)*-1\n",
    "    L_keypoints = [\"\"]*NUM_HAND_POINTS\n",
    "\n",
    "    R_x_coors = np.ones(NUM_HAND_POINTS)*-1\n",
    "    R_y_coors = np.ones(NUM_HAND_POINTS)*-1\n",
    "    R_keypoints = [\"\"]*NUM_HAND_POINTS\n",
    "\n",
    "    for annotation in item[\"annotations\"]:\n",
    "        for point in annotation[\"result\"]:\n",
    "          keypoint = point[\"value\"][\"keypointlabels\"][0]\n",
    "          if keypoint.startswith(\"L\"):\n",
    "            new_string = keypoint[len(\"L_\"):]\n",
    "            index = str_to_index(new_string)\n",
    "\n",
    "            L_keypoints[index] = new_string\n",
    "            L_x_coors[index] = point[\"value\"][\"x\"]\n",
    "            L_y_coors[index] = point[\"value\"][\"y\"]\n",
    "\n",
    "          else:\n",
    "            new_string = keypoint[len(\"R_\"):]\n",
    "            index = str_to_index(new_string)\n",
    "            # print(new_string)\n",
    "            R_keypoints[index] = new_string\n",
    "            R_x_coors[index] = point[\"value\"][\"x\"]\n",
    "            R_y_coors[index] = point[\"value\"][\"y\"]\n",
    "\n",
    "        annotations.append(Annotation(new_image_name,L_x_coors,L_x_coors,L_y_coors, \\\n",
    "                                      R_x_coors,R_x_coors,R_y_coors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_MX_TGLnNwSU",
    "outputId": "d0f85ff0-54ec-4d96-a01a-385d74a9a658"
   },
   "outputs": [],
   "source": [
    "for anno in annotations:\n",
    "  print(anno.image_name)\n",
    "  # print(anno.L_keypoint_label_arr)\n",
    "  # print(anno.R_keypoint_label_arr)\n",
    "  # print(anno.L_x_arr)\n",
    "  # print(anno.R_x_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EJFiEFPDFc41",
    "outputId": "36e52316-1681-4fa9-a326-ec093852bb8e"
   },
   "outputs": [],
   "source": [
    "# Loop over the first 5 annotations\n",
    "for i, anno in enumerate(annotations):\n",
    "    # Check if the index is less than 5\n",
    "    if i < 20:\n",
    "        # Draw keypoints for the current annotation\n",
    "        draw_keypoints(anno.image_name, anno.L_x_arr, anno.L_y_arr, anno.R_x_arr, anno.R_y_arr)\n",
    "    else:\n",
    "        # Break the loop if we've reached the 6th annotation\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8TjYWD48WPzE"
   },
   "outputs": [],
   "source": [
    "def load_image(image_name, target_size=(240, 176)):\n",
    "    # Load image using OpenCV\n",
    "    image = cv2.imread(image_name)\n",
    "\n",
    "    # Resize the image to the target size if necessary\n",
    "    if target_size is not None:\n",
    "        image = cv2.resize(image, target_size)\n",
    "\n",
    "    # Normalize pixel values to be between 0 and 1\n",
    "    image = image / 255.0\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OnMwXARnVfsx"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the annotations into train and test sets\n",
    "train_annotations, test_annotations = train_test_split(annotations, test_size=0.1, random_state=42)\n",
    "\n",
    "# Further split the train annotations into train and validation sets\n",
    "train_annotations, val_annotations = train_test_split(train_annotations, test_size=0.3, random_state=42)\n",
    "\n",
    "# Now you have train_annotations, val_annotations, and test_annotations\n",
    "# You can use these arrays to train, validate, and test your model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rztHvD_yWNqv"
   },
   "outputs": [],
   "source": [
    "# Define training data\n",
    "x_train = []  # List to store training images\n",
    "y_train = []  # List to store training annotations\n",
    "\n",
    "for annotation in train_annotations:\n",
    "    # Assuming you have a function to load images based on image names\n",
    "    image = load_image(annotation.image_name)  # Load the image\n",
    "    x_train.append(image)  # Append the image to x_train\n",
    "    concatenated_arr = np.concatenate([annotation.L_x_arr[:, np.newaxis], annotation.L_y_arr[:, np.newaxis],\n",
    "                                       annotation.R_x_arr[:, np.newaxis], annotation.R_y_arr[:, np.newaxis]], axis=-1)\n",
    "    y_train.append(concatenated_arr)\n",
    "\n",
    "# Define validation data\n",
    "x_val = []  # List to store validation images\n",
    "y_val = []  # List to store validation annotations\n",
    "\n",
    "for annotation in val_annotations:\n",
    "    # Assuming you have a function to load images based on image names\n",
    "    image = load_image(annotation.image_name)  # Load the image\n",
    "    x_val.append(image)  # Append the image to x_val\n",
    "    # Append the annotations to y_val directly, assuming Annotation object has attributes L_x_arr, L_y_arr, R_x_arr, R_y_arr\n",
    "    concatenated_arr = np.concatenate([annotation.L_x_arr[:, np.newaxis], annotation.L_y_arr[:, np.newaxis],\n",
    "                                       annotation.R_x_arr[:, np.newaxis], annotation.R_y_arr[:, np.newaxis]], axis=-1)\n",
    "\n",
    "    y_val.append(concatenated_arr)\n",
    "\n",
    "# Define testing data\n",
    "x_test = []  # List to store testing images\n",
    "y_test = []  # List to store testing annotations\n",
    "\n",
    "for annotation in test_annotations:\n",
    "    # Assuming you have a function to load images based on image names\n",
    "    image = load_image(annotation.image_name)  # Load the image\n",
    "    x_test.append(image)  # Append the image to x_test\n",
    "    # Append the annotations to y_test directly, assuming Annotation object has attributes L_x_arr, L_y_arr, R_x_arr, R_y_arr\n",
    "    concatenated_arr = np.concatenate([annotation.L_x_arr[:, np.newaxis], annotation.L_y_arr[:, np.newaxis],\n",
    "                                       annotation.R_x_arr[:, np.newaxis], annotation.R_y_arr[:, np.newaxis]], axis=-1)\n",
    "\n",
    "    y_test.append(concatenated_arr)\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "# y_train = np.swapaxes(y_train, 1, 2)\n",
    "x_val = np.array(x_val)\n",
    "y_val = np.array(y_val)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rC0rb1x-bCMn",
    "outputId": "a2132034-c626-4196-80fb-a78c13a1b6cd"
   },
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FsR0OlMXbIDC"
   },
   "outputs": [],
   "source": [
    "# copy old hand pose estimation model\n",
    "  # x = 11-->18\n",
    "  # 18 --> 21, 42\n",
    "# Define a simple neural network for hand pose estimation\n",
    "# input\n",
    "  #1 image - 176 pixels x 240 pixels x 3 rgb channels\n",
    "# output\n",
    "  # number of joints/21 joints\n",
    "def create_hand_pose_model(input_shape=(176, 240, 3), num_joints=21):\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=input_shape))\n",
    "  model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "  model.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "  model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "  model.add(tf.keras.layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "  model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "  model.add(tf.keras.layers.Conv2D(512, (3, 3), activation='relu'))\n",
    "  model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "  model.add(tf.keras.layers.Conv2D(512, (3, 3), activation='relu'))\n",
    "  model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "  # Flatten and fully connected layers\n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "  model.add(tf.keras.layers.Dense(700, activation='relu'))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  # Modify the last Dense layer for (21, 3) output\n",
    "  model.add(tf.keras.layers.Dense(num_joints * 4, activation='linear'))\n",
    "\n",
    "  # Reshape the output to (21, 3)\n",
    "  model.add(tf.keras.layers.Reshape((num_joints, 4)))       # output is 21 joints\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "hand_pose_model = create_hand_pose_model()\n",
    "hand_pose_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# hand_pose_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jVcZFXbFbwPB",
    "outputId": "6aeb8de1-bcd5-475d-b634-112d2cb8905f"
   },
   "outputs": [],
   "source": [
    "# originally epochs=50\n",
    "history = hand_pose_model.fit(x_train, y_train, epochs=50,shuffle=True,validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xXqUGjb8b63E"
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "MODEL_TF = \"hand_pose_model_tf_my_data\"\n",
    "hand_pose_model.save(MODEL_TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKhmOY1Mb8Hn"
   },
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "loaded_model = tf.keras.models.load_model(MODEL_TF)\n",
    "\n",
    "# Now you can use the loaded model for predictions or further training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "veO5E7k6ceVz",
    "outputId": "f163686c-e5ad-4015-edfd-0a5c3c1b90b3"
   },
   "outputs": [],
   "source": [
    "# generate loss curve here\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9ZGzNUJhlDU"
   },
   "outputs": [],
   "source": [
    "def draw_hand(y_train,color='cyan'):\n",
    "  zero_arr = [1,3,5,7,9]\n",
    "\n",
    "  for i in zero_arr:\n",
    "    plt.plot([y_train[0,0],y_train[i,0]],[y_train[0,1],y_train[i, 1]],color=color)\n",
    "    plt.plot([y_train[i,0],y_train[i+1,0]],[y_train[i,1],y_train[i+1, 1]],color=color)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "8S2UVKkBhlt_",
    "outputId": "379dc0f7-55ba-4ef3-d3bf-03c1bfc6e24f"
   },
   "outputs": [],
   "source": [
    "i = 2\n",
    "\n",
    "guess = hand_pose_model.predict(x_train[i].reshape(-1,176,240,3))\n",
    "\n",
    "plt.imshow(x_train[i])\n",
    "\n",
    "print(guess.shape)\n",
    "\n",
    "# draw_hand(y_train[i])\n",
    "# plt.scatter(y_train[i, :,  0], y_train[i, :, 1])\n",
    "draw_hand(guess[0],'orange')\n",
    "plt.scatter(guess[0, :,  0], guess[0, :, 1])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "BsERMzuahnk6",
    "outputId": "26de7c75-9bf1-4e8c-be87-b38616c9de44"
   },
   "outputs": [],
   "source": [
    "i = 2\n",
    "\n",
    "guess = hand_pose_model.predict(x_test[i].reshape(-1,176,240,3))\n",
    "\n",
    "plt.imshow(x_test[i])\n",
    "# plt.scatter(y_test[i, :,  0], y_test[i, :, 1])\n",
    "draw_hand(guess[0],'orange')\n",
    "plt.scatter(guess[0, :,  0], guess[0, :, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1evqr8CJCoh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
